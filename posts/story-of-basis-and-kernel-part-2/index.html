
 <!DOCTYPE HTML>
<html lang="EN">
<head>
  <meta charset="UTF-8">
  
    <title>A Story of Basis and Kernel - Part II: Reproducing Kernel Hilbert Space</title>
    <meta name="viewport" content="width=device-width, initial-scale=1,user-scalable=no">
    
    <meta name="author" content="Song, Changyue">

<meta property="og:type" content="article">
<meta property="og:title" content="A Story of Basis and Kernel - Part II: Reproducing Kernel Hilbert Space">
<meta property="og:url" content="http://songcy.net/posts/story-of-basis-and-kernel-part-2">
<meta property="og:site_name" content="Green Sky">
<meta property="og:description" content="In the previous blog, the function basis was briefly discussed. We began with viewing a function as an infinite vector, and then we defined the inner product of functions. Similar to Rn space, we can also find orthogonal function basis for a function space.">
<meta property="og:image" content="../../post-attachments/6/example1.png">
<meta name="twitter:card" content="story">
<meta name="twitter:title" content="A Story of Basis and Kernel - Part II: Reproducing Kernel Hilbert Space">
<meta name="twitter:description" content="In the previous blog, the function basis was briefly discussed. We began with viewing a function as an infinite vector, and then we defined the inner product of functions. Similar to Rn space, we can also find orthogonal function basis for a function space.">


    
    <link rel="alternative" href="/atom.xml" title="Green Sky" type="application/atom+xml">
    
    
    <link rel="icon" href="/images/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/images/frontpage.jpg">
    <link rel="apple-touch-icon-precomposed" href="/images/frontpage.jpg">
    
    <link rel="stylesheet" href="/css/style.css" type="text/css">

	<link rel="stylesheet" href="/css/flexslider.css" type="text/css">

</head>

  <body>
    <header>
      <div>
		<div id="mailboxbar">
			<span><a class="icon-envelope-alt" href="mailto:songcy08@qq.com"> songcy08@qq.com</a></span>
		</div>
		
			<div id="imglogo">
				<a href="/"><img src="/images/author.jpg" alt="Green Sky" title="Green Sky"/></a>
			</div>
			
			<div id="textlogo">
				<h1 class="site-name"><a id="blue-site-name" href="/" title="Green Sky">Green Sky</a></h1>
				<h2 class="blog-motto">Welcome to Changyue Song&#39;s Homepage</h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="Menu">
			</a></div>
			<nav id="animated">
					<ul>
					 
						<li><a href="/">Home</a></li>
					
						<li><a href="/academy">Academy</a></li>
					
						<li><a href="/favorites">Favorites</a></li>
					
						<li><a href="/travel">Travel</a></li>
					
						<li><a href="/posts">Posts</a></li>
					
						<li><a href="/leave-a-message">Leave a message</a></li>
					
					<li>
 					
					<form class="search" action="//google.com/search" method="get" accept-charset="utf-8">
						<label>Search</label>
						<input type="search" id="search" name="q" autocomplete="off" maxlength="20" placeholder="Search" />
						<input type="hidden" name="q" value="site:songcy.net">
					</form>
					
					</li>
				</ul>
			</nav>			
</div>
    </header>
    <div id="container">
      <div id="main" class="post" itemscope itemprop="blogPost">
  
	<article itemprop="articleBody"> 
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/posts/story-of-basis-and-kernel-part-2/" title="A Story of Basis and Kernel - Part II" itemprop="url">A Story of Basis and Kernel - Part II: Reproducing Kernel Hilbert Space</a>
  </h1>
  <p class="article-author">By
       
		<a href="http://songcy.net/about" title="Song, Changyue" target="_blank" itemprop="author">Song, Changyue</a>
		
  <p class="article-time">
    <time datetime="2015-10-28T20:00:00.000Z" itemprop="datePublished"> Published 2015-10-28</time>
    
  </p>
</header>
	<div class="article-content">
		
		<div id="toc" class="toc-article">
			<strong class="toc-title">Contents</strong>
		
		<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#opening-words"><span class="toc-text">1. Opening Words</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#eigen-vector"><span class="toc-text">2. Eigen Decomposition</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#kernel-function"><span class="toc-text">3. Kernel Function</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#rkhs"><span class="toc-text">4. Reproducing Kernel Hilbert Space</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#simple-example"><span class="toc-text">5. A Simple Example</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#svm"><span class="toc-text">6. Support Vector Machine</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#summary"><span class="toc-text">7. Summary and Reference</span></a></li></ol>
		
		</div>

<h2 id="opening-words">1. Opening Words </h2>
<p>In the <a href="../story-of-basis-and-kernel-part-1">previous blog</a>, the function basis was briefly discussed. We began with viewing a function as an infinite vector, and then defined the inner product of functions. Similar to <span class="math">\( \mathcal{R}^n \) </span> space, we can also find orthogonal function basis for a function space.</p>

<p>This blog will move a step further discussing about kernel functions and reproducing kernel Hilbert space (RKHS). Kernel methods have been widely used in a variety of data analysis techniques. The motivation of kernel method arises in mapping a vector in <span class="math">\( \mathcal{R}^n \) </span> space as another vector in a feature space. For example, imagine there are some red points and some blue points as the next figure shows, which are not easily separable in <span class="math">\( \mathcal{R}^n \) </span> space. However, if we map them into a high-dimension feature space, we may be able to seperate them easily. This article will not provide strict theoretical definition, but rather intuitive description on the basic ideas.</p>

<center>
<a href="../../post-attachments/6/example1.PNG"><img src="../../post-attachments/6/example1.PNG" alt="" width="600"></a>
</center>

<h2 id="eigen-vector">2. Eigen Decomposition </h2>
<p>For a real symmetric matrix <span class="math">\( \mathbf{A} \) </span>, there exists real number <span class="math">\( \lambda \) </span> and vector <span class="math">\( \mathbf{x} \) </span> so that <span class="math">\[ \mathbf{A} \mathbf{x} = \lambda \mathbf{x} \] </span> Then <span class="math">\( \lambda \) </span> is an eigenvalue of <span class="math">\( \mathbf{A} \) </span> and <span class="math">\( \mathbf{x} \) </span> is the corresponding eigenvector. If <span class="math">\( \mathbf{A} \) </span> has two different eigenvalues <span class="math">\( \lambda_1 \) </span> and <span class="math">\( \lambda_2 \) </span>, <span class="math">\( \lambda_1 \neq \lambda_2 \) </span>, with corresponding eigenvectors <span class="math">\( \mathbf{x}_1 \) </span> and <span class="math">\( \mathbf{x}_2 \) </span> respectively, <span class="math">\[ \lambda_1 \mathbf{x}_1^T \mathbf{x}_2 = \mathbf{x}_1^T \mathbf{A}^T \mathbf{x}_2 = \mathbf{x}_1^T \mathbf{A} \mathbf{x}_2 = \lambda_2 \mathbf{x}_1^T \mathbf{x}_2 \] </span> Since <span class="math">\( \lambda_1 \neq \lambda_2 \) </span>, we have <span class="math">\( \mathbf{x}_1^T \mathbf{x}_2 = 0 \) </span>, i.e., <span class="math">\( \mathbf{x}_1 \) </span> and <span class="math">\( \mathbf{x}_2 \) </span> are orthogonal.
</p>

<p>For <span class="math">\( \mathbf{A} \in \mathcal{R}^{n \times n} \) </span>, we can find <span class="math">\( n \) </span> eigenvalues a long with <span class="math">\( n \) </span> orthogonal eigenvectors. As a result, <span class="math">\( \mathbf{A} \) </span> can be decomposited as <span class="math">\[ \mathbf{A} = \mathbf{Q} \mathbf{D} \mathbf{Q}^T \] </span> where <span class="math">\( \mathbf{Q} \) </span> is an orthogonal matrix (i.e., <span class="math">\( \mathbf{Q} \mathbf{Q}^T = \mathbf{I} \) </span>) and <span class="math">\( \mathbf{D} = \mbox{diag} (\lambda_1, \lambda_2, \cdots, \lambda_n) \) </span>. If we write <span class="math">\( \mathbf{Q} \) </span> column by column <span class="math">\[ \mathbf{Q}=\left( \mathbf{q}_1, \mathbf{q}_2, \cdots, \mathbf{q}_n \right) \] </span> then <span class="math">\begin{eqnarray*} \mathbf{A}=\mathbf{Q} \mathbf{D} \mathbf{Q}^T &=& \left( \mathbf{q}_1, \mathbf{q}_2, \cdots, \mathbf{q}_n \right) \begin{pmatrix}
\lambda_1\\
&\lambda_2\\
&&\ddots\\
&&&\lambda_n\\
\end{pmatrix} \begin{pmatrix} \mathbf{q}_1^T \\ \mathbf{q}_2^T \\ \vdots \\ \mathbf{q}_n^T \end{pmatrix} \\ & = & \left( \lambda_1 \mathbf{q}_1, \lambda_2 \mathbf{q}_2, \cdots, \lambda_n \mathbf{q}_n \right) \begin{pmatrix} \mathbf{q}_1^T \\ \mathbf{q}_2^T \\ \vdots \\ \mathbf{q}_n^T \end{pmatrix} \\ & = & \sum_{i=1}^n \lambda_i \mathbf{q}_i \mathbf{q}_i^T  \end{eqnarray*} </span> Here <span class="math">\( \{ \mathbf{q}_i \}_{i=1}^n \) </span> is a set of orthogonal basis of <span class="math">\( \mathcal{R}^n \) </span>.
</p>

<h2 id="kernel-function">3. Kernel Function</h2>

<p>A function <span class="math">\( f(\mathbf{x}) \) </span> can be viewed as an infinite vector, then for a function with two independent variables <span class="math">\( K(\mathbf{x},\mathbf{y}) \) </span>, we can view it as an infinite matrix. Among them, if <span class="math">\( K(\mathbf{x},\mathbf{y}) = K(\mathbf{y},\mathbf{x}) \) </span> and <span class="math">\[ \int \int \mathbf{x} K(\mathbf{x},\mathbf{y}) \mathbf{y} d\mathbf{x} d\mathbf{y} \geq 0 \] </span> for any <span class="math">\( \mathbf{x},\mathbf{y} \) </span>, then <span class="math">\( K(\mathbf{x},\mathbf{y}) \) </span> is symmetric and positive definite, in which case <span class="math">\( K(\mathbf{x},\mathbf{y}) \) </span> is a kernel function. </p>

<p>Similar to matrix eigenvalue and eigenvector, there exists eigenvalue <span class="math">\( \lambda \) </span> and eigenfunction <span class="math">\( \psi(\mathbf{x}) \) </span> so that <span class="math">\[ \int K(\mathbf{x},\mathbf{y}) \psi(\mathbf{x}) d\mathbf{x} = \lambda \psi(\mathbf{y}) \] </span> For different eigenvalues <span class="math">\( \lambda_1 \) </span> and <span class="math">\( \lambda_2 \) </span> with corresponding eigenfunctions <span class="math">\( \psi_1(\mathbf{x}) \) </span> and <span class="math">\( \psi_2(\mathbf{x}) \) </span>, it is easy to show that <span class="math">\begin{eqnarray*} \int \lambda_1 \psi_1(\mathbf{x}) \psi_2(\mathbf{x}) d\mathbf{x} & = & \int \int K(\mathbf{y},\mathbf{x}) \psi_1(\mathbf{y}) d\mathbf{y} \psi_2(\mathbf{x}) d\mathbf{x} \\ & = & \int \int K(\mathbf{x},\mathbf{y}) \psi_2(\mathbf{x}) d\mathbf{x} \psi_1(\mathbf{y}) d\mathbf{y} \\ & = & \int \lambda_2 \psi_2(\mathbf{y}) \psi_1(\mathbf{y}) d\mathbf{y} \\ & = & \int \lambda_2 \psi_2(\mathbf{x}) \psi_1(\mathbf{x}) d\mathbf{x} \end{eqnarray*} </span> Therefore, <span class="math">\[ &lt; \psi_1, \psi_2 &gt; = \int \psi_1(\mathbf{x}) \psi_2(\mathbf{x}) d\mathbf{x} = 0 \] </span> Again, the eigenfunctions are orthogonal. Here <span class="math">\( \psi \) </span> denotes the function (the infinite vector) itself. 
</p>

<p>For a kernel function, infinite eigenvalues <span class="math">\( \{ \lambda_i \}_{i=1}^{\infty} \) </span> along with infinite eigenfunctions <span class="math">\( \{ \psi_i \}_{i=1}^{\infty} \) </span> may be found. Similar to matrix case, <span class="math">\[ K(\mathbf{x},\mathbf{y}) = \sum_{i=0}^{\infty} \lambda_i \psi_i (\mathbf{x}) \psi_i (\mathbf{y})  \] </span> which is the Mercer's theorem. Here <span class="math">\( &lt; \psi_i, \psi_j &gt; = 0 \) </span> for <span class="math">\( i \neq j \) </span>. Therefore, <span class="math">\( \{ \psi_i \}_{i=1}^{\infty} \) </span> construct a set of orthogonal basis for a function space.
</p>

<p>Here are some commonly used kernels:
<ul>
<li>Polynomial kernel <span class="math">\(  K(\mathbf{x},\mathbf{y}) = ( \gamma \mathbf{x}^T \mathbf{y} + C)^d \) </span></li>
<li>Gaussian radial basis kernel <span class="math">\( K(\mathbf{x},\mathbf{y}) = \exp (-\gamma \Vert \mathbf{x} - \mathbf{y} \Vert^2 ) \) </span></li>
<li>Sigmoid kernel <span class="math">\( K(\mathbf{x},\mathbf{y}) = \tanh (\gamma \mathbf{x}^T \mathbf{y} + C ) \) </span></li>
</ul>
</p>

<h2 id="rkhs">4. Reproducing Kernel Hilbert Space</h2>

Treat <span class="math">\( \{ \sqrt{\lambda_i} \psi_i \}_{i=1}^{\infty} \) </span> as a set of orthogonal basis and construct a Hilbert space <span class="math">\( \mathcal{H} \) </span>. Any function or vector in the space can be represented as the linear combination of the basis. Suppose <span class="math">\[ f = \sum_{i=1}^{\infty} f_i \sqrt{\lambda_i} \psi_i \] </span> we can denote <span class="math">\( f \) </span> as an infinite vector in <span class="math">\( \mathcal{H} \) </span>: <span class="math">\[ f = (f_1, f_2, ...)_\mathcal{H}^T \] </span> For another function <span class="math">\( g = (g_1, g_2, ...)_\mathcal{H}^T \) </span>, we have <span class="math">\[ &lt; f,g &gt;_\mathcal{H} = \sum_{i=1}^{\infty} f_i g_i \] </span>
</p>

<p>For the kernel function <span class="math">\(  K \)</span>, here I use <span class="math">\(  K(\mathbf{x},\mathbf{y}) \) </span> to denote the evaluation of <span class="math">\(  K \) </span> at point <span class="math">\( \mathbf{x},\mathbf{y} \) </span> which is a scalar, use <span class="math">\(  K(\cdot,\cdot) \) </span> to denote the function (the infinite matrix) itself, and use <span class="math">\(  K(\mathbf{x},\cdot) \) </span> to denote the <span class="math">\( \mathbf{x} \)</span>th "row" of the matrix, i.e., we fix one parameter of the kernel function to be <span class="math">\( \mathbf{x} \) </span> then we can regard it as a function with one parameter or as an infinite vector. Then <span class="math">\[ K(\mathbf{x},\cdot) = \sum_{i=0}^{\infty} \lambda_i \psi_i (\mathbf{x}) \psi_i \] </span> In space <span class="math">\( \mathcal{H} \) </span>, we can denote <span class="math">\[ K(\mathbf{x},\cdot) = (\sqrt{\lambda_1} \psi_1 (\mathbf{x}), \sqrt{\lambda_2} \psi_2 (\mathbf{x}), \cdots )_\mathcal{H}^T \] </span> Therefore <span class="math">\[ &lt; K(\mathbf{x},\cdot), K(\mathbf{y},\cdot) &gt;_\mathcal{H}  = \sum_{i=0}^{\infty} \lambda_i \psi_i (\mathbf{x}) \psi_i(\mathbf{y}) = K(\mathbf{x},\mathbf{y}) \] </span> This is the <em>reproducing</em> property, thus <span class="math">\( \mathcal{H} \) </span> is called reproducing kernel Hilbert space (RKHS).
</p>

<p>Now it is time to return to the problem from the beginning of this article: how to map a point into a feature space? If we define a mapping <span class="math">\[ \boldsymbol{\Phi} (\mathbf{x}) = K(\mathbf{x},\cdot) = (\sqrt{\lambda_1} \psi_1 (\mathbf{x}), \sqrt{\lambda_2} \psi_2 (\mathbf{x}), \cdots )^T \] </span> then we can map the point <span class="math">\( \mathbf{x} \) </span> to <span class="math">\(  \mathcal{H} \) </span>. Here <span class="math">\(  \boldsymbol{\Phi} \) </span> is not a function, since it points to a vector or a funtion in the feature space <span class="math">\(  \mathcal{H} \) </span>. Then <span class="math">\[ &lt; \boldsymbol{\Phi} (\mathbf{x}), \boldsymbol{\Phi} (\mathbf{y}) &gt;_\mathcal{H} = &lt; K(\mathbf{x},\cdot), K(\mathbf{y},\cdot) &gt;_\mathcal{H} = K(\mathbf{x},\mathbf{y}) \] </span> As a result, we do not need to actually know what is the mapping, where is the feature space, or what is the basis of the feature space. For a symmetric positive-definite function <span class="math">\( K  \) </span>, there must exist at least one mapping <span class="math">\( \boldsymbol{\Phi} \) </span> and one feature space <span class="math">\(  \mathcal{H} \) </span> so that <span class="math">\[ &lt; \boldsymbol{\Phi} (\mathbf{x}), \boldsymbol{\Phi} (\mathbf{y}) &gt; = K(\mathbf{x},\mathbf{y}) \] </span> which is the so-called <em>kernel trick</em>.</p>

<h2 id="simple-example">5. A Simple Example</h2>
<p>Consider kernel function <span class="math">\[ K(\mathbf{x},\mathbf{y}) = \left( x_1, x_2, x_1 x_2 \right) \begin{pmatrix} y_1 \\ y_2 \\ y_1 y_2 \end{pmatrix} = x_1 y_1 + x_2 y_2 + x_1 x_2 y_1 y_2 \] </span> where <span class="math">\( \mathbf{x}=(x_1,x_2)^T, \mathbf{y}=(y_1,y_2)^T  \) </span>. Let <span class="math">\( \lambda_1=\lambda_2=\lambda_3=1, \psi_1(\mathbf{x})=x_1, \psi_2(\mathbf{x})=x_2, \psi_3(\mathbf{x})=x_1 x_2  \) </span>. We can define the mapping as <span class="math">\[ \begin{pmatrix} x_1 \\ x_2 \end{pmatrix} \overset{\boldsymbol{\Phi}}{\longrightarrow} \begin{pmatrix} x_1 \\ x_2 \\ x_1 x_2 \end{pmatrix} \] </span> Then <span class="math">\[ &lt; \boldsymbol{\Phi} (\mathbf{x}), \boldsymbol{\Phi}(\mathbf{y})  &gt; = \left( x_1, x_2, x_1 x_2 \right) \begin{pmatrix} y_1 \\ y_2 \\ y_1 y_2 \end{pmatrix} = K(\mathbf{x},\mathbf{y}) \] </span>
</p>

<h2 id="svm">6. Support Vector Machine</h2>
<p>Support vector machine (SVM) is one of the most widely known application of RKHS. Suppose we have data pairs <span class="math">\( \{ (\mathbf{x}_i, y_i) \}_{i=1}^n \) </span> where <span class="math">\(  y_i \) </span> is either 1 or -1 denoting the class of the point <span class="math">\(  \mathbf{x}_i \)</span>. SVM assumes a hyperplane to best seperate the two classes. <span class="math">\[ \min_{\boldsymbol{\beta}, \beta_0} \frac{1}{2} \Vert \boldsymbol{\beta} \Vert^2 + C \sum_{i=1}^n \xi_i \] </span> <span class="math">\[ \mbox{subject to } \xi_i \geq 0, y_i (\mathbf{x}_i^T \boldsymbol{\beta} + \beta_0 ) \geq 1 - \xi_i, \forall i \] </span> Sometimes the two classes cannot be easily seperated in <span class="math">\( \mathcal{R}^n \) </span> space, thus we can map <span class="math">\( \mathbf{x}_i  \) </span> into a high-dimension feature space where the two classes may be easily seperated. The original problem can be reformulated as <span class="math">\[ \min_{\boldsymbol{\beta}, \beta_0} \frac{1}{2} \Vert \boldsymbol{\beta} \Vert^2 + C \sum_{i=1}^n \xi_i \] </span> <span class="math">\[ \mbox{subject to } \xi_i \geq 0, y_i (\boldsymbol{\Phi}(\mathbf{x}_i)^T \boldsymbol{\beta} + \beta_0 ) \geq 1 - \xi_i, \forall i \] </span> The Lagrange function is <span class="math">\[ L_p = \frac{1}{2} \Vert \boldsymbol{\beta} \Vert^2 + C \sum_{i=1}^n \xi_i - \sum_{i=1}^n \alpha_i [y_i (\boldsymbol{\Phi}(\mathbf{x}_i)^T \boldsymbol{\beta} + \beta_0) - (1-\xi_i)] -\sum_{i=1}^n \mu_i \xi_i \] </span> Since <span class="math">\[ \frac{\partial L_p}{\partial \boldsymbol{\beta}} = \mathbf{0} \] </span> we get <span class="math">\[ \boldsymbol{\beta} = \sum_{i=1}^n \alpha_i y_i \boldsymbol{\Phi}(\mathbf{x}_i) \] </span> That is, <span class="math">\( \boldsymbol{\beta}  \) </span> can be writen as the linear combination of <span class="math">\( \mathbf{x}_i  \)</span>s! We can substitute <span class="math">\( \boldsymbol{\beta}  \) </span> and get the new optimization problem. The objective function changes to: <span class="math">\begin{eqnarray*} \frac{1}{2} \Vert \sum_{i=1}^n \alpha_i y_i \boldsymbol{\Phi} (\mathbf{x}_i) \Vert^2 + C \sum_{i=1}^n \xi_i &=& \frac{1}{2} &lt; \sum_{i=1}^n \alpha_i y_i \boldsymbol{\Phi} (\mathbf{x}_i), \sum_{j=1}^n \alpha_j y_j \boldsymbol{\Phi} (\mathbf{x}_j) &gt; + C \sum_{i=1}^n \xi_i \\ &=& \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j &lt; \boldsymbol{\Phi} (\mathbf{x}_i), \boldsymbol{\Phi} (\mathbf{x}_j) &gt; + C \sum_{i=1}^n \xi_i \\ & = & \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j K(\mathbf{x}_i, \mathbf{x}_j) + C \sum_{i=1}^n \xi_i \end{eqnarray*} </span> The constraints changes to: <span class="math">\begin{eqnarray*} y_i \left[\boldsymbol{\Phi}(\mathbf{x}_i)^T \left( \sum_{j=1}^n \alpha_j y_j \boldsymbol{\Phi}(\mathbf{x}_j) \right) + \beta_0 \right] &=& y_i \left[ \left( \sum_{j=1}^n \alpha_j y_j &lt; \boldsymbol{\Phi}(\mathbf{x}_i), \boldsymbol{\Phi}(\mathbf{x}_j) &gt; \right) + \beta_0 \right] \\ &=& y_i \left[ \left( \sum_{j=1}^n \alpha_j y_j K(\mathbf{x}_i, \mathbf{x}_j) \right) + \beta_0 \right] \geq 1 - \xi_i, \forall i \end{eqnarray*} </span> What we need to do is determining a kernel function and solve for <span class="math">\( \boldsymbol{\alpha}, \beta_0, \xi_i  \) </span>. We do not need to actually construct the feature space. For a new data <span class="math">\(  \mathbf{x} \) </span> with known class, we can predict its class by <span class="math">\begin{eqnarray*} \hat{y} &=& \mbox{sign} \left[ \boldsymbol{\Phi} (\mathbf{x})^T \boldsymbol{\beta} + \beta_0 \right] \\ &=& \mbox{sign} \left[ \boldsymbol{\Phi} (\mathbf{x})^T \left( \sum_{i=1}^n \alpha_i y_i \boldsymbol{\Phi}(\mathbf{x}_i) \right) + \beta_0 \right] \\ &=& \mbox{sign} \left( \sum_{i=1}^n \alpha_i y_i &lt; \boldsymbol{\Phi} (\mathbf{x}), \boldsymbol{\Phi}(\mathbf{x}_i) &gt;  + \beta_0 \right) \\ &=& \mbox{sign} \left( \sum_{i=1}^n \alpha_i y_i K(\mathbf{x},\mathbf{x}_i) + \beta_0 \right) \end{eqnarray*} </span> Kernel methods greatly strengthen the discriminative power of SVM.
</p>

<h2 id="summary">7. Summary and Reference</h2>

<p>Kernel method has been widely utilized in data analytics. Here, the fundamental property of RKHS is introduced. With kernel trick, we can easily map the data to a feature space and do analysis. Here is a vedio with nice demonstration on why we can easily do classification with kernel SVM in a high-dimension feature space. </p>
<iframe width="420" height="315" src="https://www.youtube.com/embed/3liCbRZPrZA" frameborder="0" allowfullscreen></iframe>
<p>The example in Section 5 is from <ul> <li> Gretton A. (2015): Introduction to RKHS, and some simple kernel algorithms, Advanced Topics in Machine Learning, Lecture conducted from University College London. </li> </ul> Other reference includes <ul><li>Paulsen, V. I. (2009). An introduction to the theory of reproducing kernel Hilbert spaces. Lecture Notes.</li><li>Daumé III, H. (2004). From zero to reproducing kernel hilbert spaces in twelve pages or less.</li><li>Friedman, J., Hastie, T., and Tibshirani, R. (2001). The elements of statistical learning. Springer, Berlin: Springer series in statistics.</li></ul></p>


  
	</div>
		<footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/academic-theory/">academic theory</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/Kernel-method/">kernel method</a><a href="/tags/function-basis/">function basis</a>
  </div>

</div>



	<div class="article-share" id="share">
	
	  <div data-url="http://songcy.net/posts/story-of-basis-and-kernel-part-2" data-title="A Story of Basis and Kernel - Part II: Reproducing Kernel Hilbert Space | Green Sky" data-tsina="null" class="share clearfix">
	  </div>
	
	</div>


</footer>

   	       
	</article>
	
<nav class="article-nav clearfix">
 
 <div class="next" >
 <a href="/posts/story-of-basis-and-kernel-part-1/" title="A Story of Basis and Kernel - Part I: Function Basis">
  <strong>下一篇：</strong><br/>
  <span>
  A Story of Basis and Kernel - Part I: Function Basis</span>
</a>
</div>

</nav>

	
<section id="comments" class="comment">
	<div class="ds-thread" data-thread-key="posts/story-of-basis-and-kernel-part-2/" data-title="A Story of Basis and Kernel - Part II: Reproducing Kernel Hilbert Space" data-url="http://songcy.net/posts/story-of-basis-and-kernel-part-2"></div>
</section>


<section id="comments" class="comment">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>

</div>  
      <div class="openaside"><a class="navbutton" href="#" title="Show Sidebar"></a></div>

  <div id="toc" class="toc-aside">
  <strong class="toc-title">Contents</strong>
 
<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#opening-words"><span class="toc-text">1. Opening Words</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#eigen-vector"><span class="toc-text">2. Eigen Decomposition</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#kernel-function"><span class="toc-text">3. Kernel Function</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#rkhs"><span class="toc-text">4. Reproducing Kernel Hilbert Space</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#simple-example"><span class="toc-text">5. A Simple Example</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#svm"><span class="toc-text">6. Support Vector Machine</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#summary"><span class="toc-text">7. Summary and Reference</span></a></li></ol>
 
  </div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="Hide Sidebar"></a></div>
<aside class="clearfix">

  
<div class="categorieslist">
	<p class="asidetitle">Categories</p>
		<ul>
		
			<li><a href="/categories/academic-practice/" title="academic practice">academic practice<sup>1</sup></a></li>
		
			<li><a href="/categories/academic-theory/" title="academic theory">academic theory<sup>5</sup></a></li>
		
			<li><a href="/categories/others/" title="others">others<sup>2</sup></a></li>
		
			<li><a href="/categories/technique/" title="technique">technique<sup>2</sup></a></li>
		
		</ul>
</div>


  
<div class="tagslist">
	<p class="asidetitle">Tags</p>
		<ul class="clearfix">
		
			<li><a href="/tags/Fourier-transform/" title="Fourier transform">Fourier transform<sup>3</sup></a></li>
		
			<li><a href="/tags/Battle-City/" title="Battle City">Battle City<sup>1</sup></a></li>
		
			<li><a href="/tags/visual-object-tracking/" title="visual object tracking">visual object tracking<sup>1</sup></a></li>
		
			<li><a href="/tags/wordpress/" title="wordpress">wordpress<sup>1</sup></a></li>
		
			<li><a href="/tags/kernel-method/" title="kernel method">kernel method<sup>2</sup></a></li>
		
			<li><a href="/tags/function-basis/" title="function basis">function basis<sup>3</sup></a></li>
		
			<li><a href="/tags/Bayesian-filter/" title="Bayesian filter">Bayesian filter<sup>1</sup></a></li>
		
			<li><a href="/tags/state-space-model/" title="state-space model">state-space model<sup>1</sup></a></li>
		
			<li><a href="/tags/power-spectral/" title="power spectral">power spectral<sup>1</sup></a></li>
		
			<li><a href="/tags/HDFS/" title="HDFS">HDFS<sup>1</sup></a></li>
		
			<li><a href="/tags/big-data/" title="big data">big data<sup>1</sup></a></li>
		
			<li><a href="/tags/Hadoop/" title="Hadoop">Hadoop<sup>1</sup></a></li>
		
			<li><a href="/tags/IERA/" title="IERA">IERA<sup>1</sup></a></li>
		
			<li><a href="/tags/ARMA/" title="ARMA">ARMA<sup>1</sup></a></li>
		
			<li><a href="/tags/periodogram/" title="periodogram">periodogram<sup>1</sup></a></li>
		
			<li><a href="/tags/particle-filter/" title="particle filter">particle filter<sup>1</sup></a></li>
		
			<li><a href="/tags/MapReduce/" title="MapReduce">MapReduce<sup>1</sup></a></li>
		
		</ul>
</div>


  <div class="linkslist">
  <p class="asidetitle">Links</p>
    <ul>
        
          <li>
            
            	<a href="http://kaibo.ie.wisc.edu" target="_blank" title="Prof. Liu, Kaibo">Prof. Liu, Kaibo</a>
            
          </li>
        
          <li>
            
            	<a href="http://iera.name" target="_blank" title="Industrial Engineering Era">IERA</a>
            
          </li>
        
          <li>
            
            	<a href="http://www.xueningzhu.com/" target="_blank" title="Zhu, Xuening">Zhu, Xuening</a>
            
          </li>
        
    </ul>
</div>

  <div class="rsspart">
	<a href="/atom.xml" target="_blank" title="rss">RSS</a>
</div>

</aside>
</div>
    </div>
    <footer><div id="footer" >
	
	
	<div class="social-font" class="clearfix">
		
		
		
		
		
		
		
		
		
		
	</div>
		<p class="copyright">Powered by <a href="http://zespia.tw/hexo/" target="_blank" title="hexo">hexo</a> and Theme by <a href="https://github.com/wuchong/jacman" target="_blank" title="Pacman">Jacman</a> © 2015 
		
		<a href="http://songcy.net" target="_blank" title="Song, Changyue">Song, Changyue</a>
		
		</p>
</div>
</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>

<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else
    {
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
      
      $('#toc.toc-aside').css('display', 'none');
        
    }
  });
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      h  = $('article h2')
      ah = $('article h2'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  if(ah.length==0){
    t.css('display','none');
  }else{
    c.click(function(){
      ta.css('display', 'block').addClass('fadeIn');
    });
    o.click(function(){
      ta.css('display', 'none');
    });
    $(window).scroll(function(){
      ta.css("top",Math.max(140,320-$(this).scrollTop()));
    });
  };
});
</script>


<script type="text/javascript">
$(document).ready(function(){ 
  var $this = $('.share'),
      url = $this.attr('data-url'),
      encodedUrl = encodeURIComponent(url),
      title = $this.attr('data-title'),
      tsina = $this.attr('data-tsina'),
      description = $this.attr('description');
  var html = [
  '<a href="#" class="overlay" id="qrcode"></a>',
  '<div class="qrcode clearfix"><span>扫描二维码分享到微信朋友圈</span><a class="qrclose" href="#nothing"></a><strong>Loading...Please wait</strong><img id="qrcode-pic" data-src="http://s.jiathis.com/qrcode.php?url=' + encodedUrl + '"/></div>',
  '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
  '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
  '<a href="#qrcode" class="article-share-qrcode" title="微信"></a>',
  '<a href="http://widget.renren.com/dialog/share?resourceUrl=' + encodedUrl + '&srcUrl=' + encodedUrl + '&title=' + title +'" class="article-share-renren" target="_blank" title="人人"></a>',
  '<a href="http://service.weibo.com/share/share.php?title='+title+'&url='+encodedUrl +'&ralateUid='+ tsina +'&searchPic=true&style=number' +'" class="article-share-weibo" target="_blank" title="微博"></a>',
  '<span title="Share to"></span>'
  ].join('');
  $this.append(html);
  $('.article-share-qrcode').click(function(){
    var imgSrc = $('#qrcode-pic').attr('data-src');
    $('#qrcode-pic').attr('src', imgSrc);
    $('#qrcode-pic').load(function(){
        $('.qrcode strong').text(' ');
    });
  });
});     
</script>



<script type="text/javascript">
  var duoshuoQuery = {short_name:"songcy"};
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
    || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
</script> 


<script type="text/javascript">

var disqus_shortname = 'songcy';

(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());
(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());
</script>






<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.article-content').each(function(i){
    $(this).find('img').each(function(){
      if ($(this).parent().hasClass('fancybox')) return;
      var alt = this.alt;
      if (alt) $(this).after('<span class="caption">' + alt + '</span>');
      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox"></a>');
    });
    $(this).find('.fancybox').each(function(){
      $(this).attr('rel', 'article' + i);
    });
  });
  if($.fancybox){
    $('.fancybox').fancybox();
  }
}); 
</script>



<!-- Analytics Begin -->





<!-- Analytics End -->

<!-- Totop Begin -->

	<div id="totop">
	<a title="Back to Top"><img src="/images/scrollup.png"/></a>
	</div>
	<script src="/js/totop.js"></script>

<!-- Totop End -->

<!-- MathJax Begin -->
<!-- mathjax config similar to math.stackexchange -->


<!-- MathJax End -->

  <!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</body>
</html>
